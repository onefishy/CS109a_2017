\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\usepackage[vlined]{algorithm2e}
\usepackage{times}
\usepackage{calc}
\usepackage{url}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{multirow, hhline}
\usepackage{array, booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{pagecolor}
\usepackage{lipsum}
\usepackage{capt-of}
\usepackage{booktabs}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage[T1]{fontenc}
\usepackage{ae}
\graphicspath{{fig/}}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]

\usetheme{default}
\beamertemplatenavigationsymbolsempty
\hypersetup{pdfpagemode=UseNone}

\usefonttheme{professionalfonts}
\usefonttheme{serif}
\usepackage{fontspec}
\setmainfont{Karla}
\setbeamerfont{note page}{family*=pplx,size=\footnotesize} % Palatino for notes

\definecolor{foreground}{RGB}{70,70,70}
\definecolor{background}{RGB}{249, 249, 249} %24,24,24
%\definecolor{title}{RGB}{107,174,214} %107,174,214
\definecolor{title}{RGB}{70,70,70}
\definecolor{gray}{RGB}{0,0,0}
\definecolor{subtitle}{RGB}{70,70,70}
\definecolor{hilight}{RGB}{102,255,204}
\definecolor{vhilight}{RGB}{255,111,207}

\setbeamercolor{titlelike}{fg=title}
\setbeamercolor{subtitle}{fg=subtitle}
\setbeamercolor{institute}{fg=gray}
\setbeamercolor{normal text}{fg=foreground,bg=background}


\setbeamercolor{item}{fg=foreground} % color of bullets
\setbeamercolor{subitem}{fg=gray}
\setbeamercolor{itemize/enumerate subbody}{fg=gray}
\setbeamertemplate{itemize subitem}{{\textendash}}
\setbeamerfont{itemize/enumerate subbody}{size=\footnotesize}
\setbeamerfont{itemize/enumerate subitem}{size=\footnotesize}

\setbeamercolor{block title}{fg=white,bg=gray!70}
\setbeamercolor{block body}{fg=black,bg=gray!10}
\setbeamercolor{block title alerted}{fg=red,bg=gray!40}
\setbeamercolor{block title example}{fg=black,bg=green!20}
\setbeamercolor{block body example}{fg=black,bg=green!5}
\setbeamerfont{block title}{series=\bfseries}

\hypersetup{colorlinks,linkcolor=foreground,urlcolor=foreground}


\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

\addtobeamertemplate{note page}{\setlength{\parskip}{12pt}}


\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ig}{\includegraphics}
\newcommand{\subt}[1]{{\footnotesize \color{subtitle} {#1}}}

\let\emph\relax % there's no \RedeclareTextFontCommand
\DeclareTextFontCommand{\emph}{\bfseries\em}


\setbeamertemplate{frametitle}
{\vskip4pt
  \leavevmode
%\hbox{%
\begin{beamercolorbox}[wd=\paperwidth,ht=2ex,dp=0ex]{frametitle}%
\underline{\makebox[\paperwidth][l]{\hspace*{10pt}
\large {{\insertframetitle}}}}
\end{beamercolorbox}
%  }%
}

%\setbeamercolor{frametitle}{fg=yellow,bg=red}

\begin{document}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \underline{\makebox[0.8\paperwidth][l]{
\large {{\insertsectionhead}}}}
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\title{\large{Lecture \#3: Multiple Linear Regression}}
\subtitle{CS 109A, STAT 121A, AC 209A: Data Science}
\author{Pavlos Protopapas \and Kevin Rader}
%\institute{Harvard University}
\date{}
\titlegraphic{
   \includegraphics[height=2cm]{iacs}\includegraphics[height=2cm]{hogwarts}
}
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage
  
}
}


\begin{frame}{Lecture Outline}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Review}

%%%%%%%%%%%%%%
\begin{frame}{Statistical Models} 
\vskip-0.4cm
We will assume that the response variable, $Y$, relates to the predictors, $X$, through some unknown function expressed generally as:
\[
Y = f(X) + \epsilon,
\]
where $\epsilon$ is a random variable representing measurement noise.
\vskip0.2cm
A \emph{statistical model} is any algorithm that estimates the function $f$. We denote the estimated function as $\widehat{f}$ and the predicted value of $Y$ given $X = x_i$ as $\widehat{y}_i$. 
\vskip0.2cm
When performing \emph{inference}, we compute parameters of $\widehat{f}$ that minimizes the error of our model, where error is measured by a choice of \emph{loss function}.
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Simple Linear Regression} 
\vskip-0.4cm
A \emph{simple linear regression model} assume that our statistical model is 
\[
Y = f(X) + \epsilon = \beta^{\text{true}}_1 X + \beta^{\text{true}}_0 + \epsilon,
\]
then it follows that $\widehat{f}$ must look like
\[
\widehat{f}(X) = \widehat{\beta_1} X +\widehat{\beta_0}.
\]
When fitting our model, we find $\widehat{\beta}_0, \widehat{\beta}_1$ to minimize the loss function, for example, 
\[
\widehat{\beta}_0, \widehat{\beta}_1 = \underset{\beta_0, \beta_1}{\mathrm{argmin}}L(\beta_0, \beta_1).
\]
The line $\widehat{Y} =  \widehat{\beta_1} X +\widehat{\beta_0}$ is called the \emph{regression line}.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More on Model Evaluation}

%%%%%%%%%%%%%%
\begin{frame}{Loss Functions Revisited} \small
\vskip-0.4cm
Recall that there are multiple ways to measure the fitness of a model, i.e. there are multiple \emph{loss functions}.
\begin{enumerate}
\small
\item (\textbf{Max absolute deviation}) Count only the biggest `error'
\[
\max_i |y_i - \widehat{y}_i| 
\]
\item (\textbf{Sum of absolute deviations}) Add up the `errors'
\[
\sum_i |y_i - \widehat{y}_i| \quad \text{or} \quad \frac{1}{n}\sum_i |y_i - \widehat{y}_i|
\]
\item (\textbf{Sum of squared errors}) Add up the squared `errors'
\[
\sum_i |y_i - \widehat{y}_i|^2 \quad \text{or} \quad \frac{1}{n} \sum_i |y_i - \widehat{y}_i|^2
\]
The average squared error is the \textbf{Mean Squared Error}.
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Model Fitness: $R^2$} 
\vskip-0.4cm
While loss functions measure the predictive errors made by a model, we are also interested in the ability of our models to capture interesting features or variations in the data. 
\vskip0.2cm
We compute the \emph{explained variance} or $R^2$, the ratio of the variation of the model and the variation in the data. The explained variance of a regression line is given by
\[
R^2 = 1 -  \frac{\sum_{i=1}^n \left| y_i -\overline{y}_i\right|^2}{\sum_{i=1}^n \left| \hat{y}_i -\overline{y}_i\right|^2}
\]
For a regression line, we have that 
\[
0\leq R^2\leq 1
\]
Can you see why?
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Model Evaluation: Standard Errors}
\vskip-0.4cm
\only<1>{
Rather than evaluating the predictive powers of our model or the explained variance, we can evaluate how confident we are in our estimates, $\widehat{\beta}_0, \widehat{\beta}_1$, of the model parameters.
\vskip0.4cm
Recall that our estimates $\widehat{\beta}_0, \widehat{\beta}_1$ will vary depending on the observed data. Thus, the variance of $\widehat{\beta}_0, \widehat{\beta}_1$ indicates the extend to which we can rely on any given estimate of these parameters.
\vskip0.4cm
The variance of $\widehat{\beta}_0, \widehat{\beta}_1$ are also called their \emph{standard errors}.}
\only<2>{
If our data is drawn from a larger set of observations then we can empirically estimate the standard errors of $\widehat{\beta}_0, \widehat{\beta}_1$ through bootstrapping. 
\vskip0.4cm
If we know the variance $\sigma^2$ of the noise $\epsilon$, we can compute $SE\left(\widehat{\beta}_0\right), SE\left(\widehat{\beta}_1\right)$ analytically, using the formulae we derived in the last lecture for $\widehat{\beta}_0, \widehat{\beta}_1$:
\begin{align*}
SE\left(\widehat{\beta}_0\right) &= \sigma\sqrt{\frac{1}{n} + \frac{\overline{x}^2}{\sum_i \left(x_i - \overline{x}\right)^2}}\\
SE\left(\widehat{\beta}_1\right) &= \frac{\sigma}{\sqrt{\sum_i\left(x_i - \overline{x}\right)^2}}
\end{align*}
}
\only<3>{
In practice, we do not know the theoretical value of $\sigma^2$, since we do not know the exact distribution of the noise $\epsilon$. However, if we make the following assumptions, 
\vskip0.2cm
\begin{itemize}
\item the errors $\epsilon_i = y_i - \widehat{y}_i$ and $\epsilon_j = y_j - \widehat{y}_j$ are uncorrelated, for $i\neq j$,
\vskip0.4cm
\item each $\epsilon_i$ is normally distributed with mean 0 and variance $\sigma^2$,
\end{itemize}
\vskip0.2cm
then, we can empirically estimate $\sigma^2$ from the data and our regression line:
\[
\sigma \approx \sqrt{\frac{n \cdot MSE}{n - 2}} = \sqrt{\frac{\sum_i\left( y_i - \widehat{y}_i\right)^2}{n - 2}}.
\]
}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Model Evaluation: Confidence Intervals}
\vskip-0.4cm
\begin{block}{Definition}
A \emph{$n\%$ confidence interval} of an estimate $\widehat{X}$ is the range of values such that the true value of $X$ is contained in this interval with $n$ percent probability. 
\end{block}
\vskip0.2cm
For linear regression, the 95\% confidence interval for $\widehat{\beta}_0, \widehat{\beta}_1$ can be approximated using their standard errors:
\[
\widehat{\beta}_k = \widehat{\beta}_k \pm 2SE\left(\widehat{\beta}_k \right)
\]
for $k = 0, 1$. Thus, with approximately 95\% probability, the true value of $\widehat{\beta}_k$ is contained in the interval $\left[\widehat{\beta}_k - 2SE\left(\widehat{\beta}_k \right), \widehat{\beta}_k + 2SE\left(\widehat{\beta}_k \right)\right]$.
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Model Evaluation: Residual Analysis}
\vskip-0.4cm
When we estimated the variance of $\epsilon$, we assumed that the residuals $\epsilon_i = y_i - \widehat{y}_i$ were uncorrelated and normally distributed with mean 0 and fixed variance.
\vskip0.2cm
These assumptions need to be verified using the data. In residual analysis, we typically create two types of plots:
\begin{enumerate}
\item a plot of $\epsilon_i$ with respect to $x_i$. This allows us to compare the distribution of the noise at different values of $x_i$.
\item a histogram of $\epsilon_i$. This allows us to explore the distribution of the noise independent of $x_i$. 
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{A Simple Example}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Linear Regression}

%%%%%%%%%%%%%%
\begin{frame}{Multilinear Models} 
\vskip-0.4cm
In practice, it is unlikely that any response variable $Y$ depends solely on one predictor $x$. Rather, we expect that $Y$ is a function of multiple predictors $f(X_1, \ldots, X_J)$. 
\vskip0.2cm
In this case, we can still assume a simple form for $f$ - a multilinear form:
\[
y = f(X_1, \ldots, X_J) + \epsilon = \beta_0 + \beta_1 x_1 + \ldots + \beta_J x_J + \epsilon.
\]
Hence, $\widehat{f}$ has the form
\[
\widehat{y} = \widehat{f}(X_1, \ldots, X_J)  =  \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_J x_J.
\]
Again, to fit this model means to compute $\widehat{\beta}_0, \ldots, \widehat{\beta}_J$ to minimize a loss function; we will again choose the MSE as our loss function.
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Multiple Linear Regression} \small
\vskip-0.4cm
Given a set of observations
\[ 
\{(x_{1, 1}, \ldots, x_{1, J}, y_1), \ldots (x_{n, 1}, \ldots, x_{n, J}, y_n)\}, 
\]
the data and the model can be expressed in vector notation,
\begin{align*}
\mathbf{Y} &= \left( \begin{array}{c}
y_1\\
\vdots\\
y_y
\end{array} \right),& \mathbf{X} &= \left( \begin{array}{cccc}
1 & x_{1, 1} & \ldots & x_{1, J}\\
1 & x_{2, 1} & \ldots & x_{2, J}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{n, 1} & \ldots & x_{n, J}
\end{array} \right), & \pmb{\beta} &=  \left( \begin{array}{c}
\beta_0\\
\beta_1\\
\vdots\\
\beta_J
\end{array} \right),
\end{align*}
Thus, the MSE can be expressed in vector notation as
\[
\mathrm{MSE}(\pmb{\beta}) = \frac{1}{n}\|\boldsymbol{Y - X\beta}\|^2.
\]
Minimizing the MSE using vector calculus yields,
\[
\widehat{\pmb{\beta}} = \left( \mathbf{X}^\top  \mathbf{X} \right)^{-1} \mathbf{X}^\top  \mathbf{Y} = \underset{\pmb{\beta}}{\mathrm{argmin}}\; \mathrm{MSE}(\pmb{\beta}).
\]
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{A Simple Example}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluating Significance of Predictors}

%%%%%%%%%%%%%%
\begin{frame}{Finding Significant Predictors: Hypothesis Testing} 
\only<1>{
With multiple predictors, an obvious analysis is to check which predictor or group of predictors have a `significant' impact on the response variable. 
\vskip0.4cm
One way to do this is to analyze the `likelihood' that any one or any set of regression coefficient is zero. Significant predictors will have coefficients that are deemed less `likely' to be zero.
\vskip0.4cm
Unfortunately, since the regression coefficient vary depending on the data, we cannot simply pick out non-zero coefficients from our estimate $\pmb{\beta}$.
}
\only<2>{
\footnotesize
\vskip-0.4cm
\begin{block}{Hypothesis Testing}
\emph{Hypothesis testing} is a formal process through which we evaluate the validity of a statistical hypothesis by considering evidence for or against the hypothesis gathered by random sampling of the data. 
\begin{enumerate}
\item State the hypotheses, typically a \emph{null hypothesis}, $H_0$, and an \emph{alternative hypothesis}, $H_1$, that is the negation of the former. 
\item Choose a type of analysis, i.e. how to use sample data to evaluate the null hypothesis. Typically this involves choosing a single test statistic.
\item Sample data and compute the test statistic.
\item Use the value of the test statistic to either reject or not reject the null hypothesis.
\end{enumerate}
\end{block}
}
\only<3>{\footnotesize
\vskip-0.4cm
For checking the significance of linear regression coefficients:
\begin{enumerate}
\item We set up our hypotheses
\begin{align*}
H_0&: \beta_0 = \beta_1 = \ldots = \beta_J = 0&& \textbf{(Null)}\\
H_1&: \beta_j \neq 0, \text{ for at least one $j$} && \textbf{(Alternative)}
\end{align*}
\item we choose the $F$-stat to evaluate the null hypothesis, 
\[
F = \frac{\text{explained variance}}{\text{unexplained variance}}
\]
\item we can compute the $F$-stat for linear regression models by
\[
F = \frac{(\mathrm{TSS} - \mathrm{RSS}) / J}{\mathrm{RSS} / (n - J - 1)}, \quad \mathrm{TSS} = \sum_i \left( y_i - \overline{y}\right), \mathrm{RSS} = \sum_i \left( y_i - \widehat{y}_i \right)
\]
\item If $F = 1$ we consider this evidence for $H_0$; if $F > 1$, we consider this evidence against $H_0$. 
\end{enumerate}
}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{More on Hypothesis Testing} 
\only<1>{
Applying the $F$-stat test to $\{ X_1, \ldots, X_J\}$ determines if any of the predictors have a significant relationship with the response. 
\vskip0.4cm
We can also apply the test to a subset of predictors to determine if a smaller group of predictors have a significant relationship with the response.
\vskip0.4cm
\textbf{Note:} There is not a fixed threshold for rejecting the null hypothesis based on the $F$-stat. 
\vskip0.2cm
For $n$ and $J$ that are large, $F$ values that are slightly above 1 are considered to be strong evidence against $H_0$.}
\only<2>{
\vskip-0.4cm
To determine if any single predictor has a significant relationship with the response, we can again perform hypothesis testing. In this case, the test statistics we use is typically the $p$-value.
\vskip0.2cm
\begin{block}{Definition}
The \emph{$p$-value} is the probability that, when the null hypothesis is true, the statistical summary of a given model would be the same as or more extreme than the observed results.
\end{block}
\vskip0.2cm
Smaller $p$-values are interpreted to be evidence against the null hypothesis. A standard $p$-value threshold for rejecting the null hypothesis is 0.05 (or 5\%).
}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Finding Significant Predictors: $R^2$} 
We can compare the `significance' of two specific groups of predictors $\{X_{j_1}, \ldots, X_{j_k}\}$ and $\{X_{j'_1}, \ldots, X_{j'_{k'}}\}$, by comparing the $R^2$ values of the two models constructed using each set
\[
R^2\left(\widehat{f}(X_{j_1}, \ldots, X_{j_k})\right) \quad \text{v.s.} \quad R^2\left(\widehat{f}(X_{j'_1}, \ldots, X_{j'_{k'}})\right)
\]
We may conclude that a higher $R^2$ (i.e. a model that fits the observation better) is evidence that one set of predictors impacts the response more significantly than the other.
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Finding Significant Predictors: Information Criteria} 
\only<1>{
Yet another way to evaluate the explanatory power of different sets of predictors is to use \emph{information criteria}. These are a set of metrics that measures the fit of the model to observations given the number of parameters used in the model.
\vskip0.2cm
Below are two different such criteria, \emph{Aiken's Information Criterion} and \emph{Bayes Information Criterion}
\begin{align*}
\mathrm{AIC} &\approx n \cdot \ln(\mathrm{RSS}/n) + 2 J\\
\mathrm{BIC} &\approx n \cdot \ln(\mathrm{RSS}/n) + J\cdot \ln(n)
\end{align*}
From the above, we can see that the smaller the AIC or BIC, the better the model. 
}
\only<2>{
We can compare the `significance' of two specific groups of predictors $\{X_{j_1}, \ldots, X_{j_k}\}$ and $\{X_{j'_1}, \ldots, X_{j'_{k'}}\}$, by comparing the AIC or BIC values of the two models constructed using each set
\[
\mathrm{AIC/BIC}\left(\widehat{f}(X_{j_1}, \ldots, X_{j_k})\right) \; \text{v.s.} \; \mathrm{AIC/BIC}\left(\widehat{f}(X_{j'_1}, \ldots, X_{j'_{k'}})\right)
\]
We may conclude that a lower AIC or BIC (i.e. a model that fits the observation better) is evidence that one set of predictors impacts the response more significantly than the other.
}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Which Metric of Significance Should We Use?} 
The procedure of systematically choosing a set of predictors that have a significant relationship with the response variable is called \emph{variable selection}. 
\vskip0.2cm
But which metric ($F$-stats, $p$-values, $R^2$, AIC/BIC) should we use to determine the significance of a set of predictors? 
\vskip0.2cm
In later lectures, we will see that each metric has its strengths and draw-backs. Rather than relying on a single metric, we should use multiple metrics in conjunction and double check with common sense!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Regression with Interaction Terms}

%%%%%%%%%%%%%%
\begin{frame}{Interacting Predictors} 
\vskip-0.4cm
\only<1>{
In our multiple linear regression model for the NYC taxi data, we considered two predictors, rush hour indicator $x_1$ (in 0 or 1) and trip length $x_2$ (in minutes),
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2.
\]
This model assumes that each predictor has an independent effect on the response, e.g. regardless of the time of day, the fare depends on the length of the trip in the same way. 
\vskip0.2cm
In reality, we know that a 30 minute trip covers a shorter distance during rush hour than in normal traffic. 
}
\only<2>{
A better model considers how the interactions between the two predictors impact the response,
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2.
\]
The term $\beta_3 x_1 x_2$ is called the \emph{interaction term}. It determines the effect on the response when we consider the predictors jointly. 
\vskip0.2cm
For example, the effect of trip length on cab fare in the absence of rush hour is $\beta_2 x_2$. When combined with rush hour traffic, the effect of trip length is $\beta_2 + \beta_3 x_2$. 
}
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Multiple Linear Regression with Interaction Terms} \small
\vskip-0.4cm
Multiple linear regression with interaction terms can be treated like a special form of multiple linear regression - we simply treat the cross terms (e.g. $x_1x_2$) as additional predictors.
\vskip0.2cm
Given a set of observations
$
\{(x_{1, 1}, x_{1, 2}, y_1), \ldots (x_{n, 1},  x_{n, 2}, y_n)\}, 
$
the data and the model can be expressed in vector notation,
\begin{align*}
\mathbf{Y} &= \left( \begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array} \right),& \mathbf{X} &= \left( \begin{array}{cccc}
1 & x_{1, 1} & x_{1, 2} & x_{1, 1} x_{1, 2}\\
1 & x_{2, 1} & x_{2, 2} & x_{2, 1} x_{2, 2}\\
\vdots & \vdots & \vdots & \vdots\\
1 & x_{n, 1} &x_{n, 2} & x_{n, 1} x_{n, 2}
\end{array} \right), & \pmb{\beta} &=  \left( \begin{array}{c}
\beta_0\\
\beta_1\\
\beta_2\\
\beta_3\\
\end{array} \right),
\end{align*}
Again, minimizing the MSE using vector calculus yields,
\[
\widehat{\pmb{\beta}} = \left( \mathbf{X}^\top  \mathbf{X} \right)^{-1} \mathbf{X}^\top  \mathbf{Y} = \underset{\pmb{\beta}}{\mathrm{argmin}}\; \mathrm{MSE}(\pmb{\beta}).
\]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Polynomial Regression}

%%%%%%%%%%%%%%
\begin{frame}{Polynomial Regression as Linear Regression} \footnotesize
\vskip-0.4cm
The simplest non-linear model we can consider, for a response $Y$ and a predictor $X$, is a polynomial model of degree $M$,
\[
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_M x^M + \epsilon.
\]
Just as in the case of linear regression with cross terms, polynomial regression is a special case of linear regression - we treat each $x^m$ as a separate predictor. Thus, we can write
\begin{align*}
\mathbf{Y} &= \left( \begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array} \right),& \mathbf{X} &= \left( \begin{array}{cccc}
1 & x^1_{1} & \ldots &  x^M_{1}\\
1 & x^1_{2} & \ldots &  x^M_{2}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{n} &\ldots & x^M_{n}
\end{array} \right), & \pmb{\beta} &=  \left( \begin{array}{c}
\beta_0\\
\beta_1\\
\vdots\\
\beta_M\\
\end{array} \right).
\end{align*}
Again, minimizing the MSE using vector calculus yields,
\[
\widehat{\pmb{\beta}} = \left( \mathbf{X}^\top  \mathbf{X} \right)^{-1} \mathbf{X}^\top  \mathbf{Y} = \underset{\pmb{\beta}}{\mathrm{argmin}}\; \mathrm{MSE}(\pmb{\beta}).
\]
\end{frame}

%%%%%%%%%%%%%%
\begin{frame}{Generalized Polynomial Regression} \small
\vskip-0.4cm
We can generalize polynomial models:
\begin{enumerate}
\item considering polynomial models with multiple predictors $\{X_1, \ldots, X_J\}$:
\begin{align*}
y = &\beta_0 + \beta_1 x_1 + \ldots + \beta_M x^M_1 \\
&+ \ldots \\
&+ \beta_{1 + MJ} x_J + \ldots + \beta_{M + MJ} x^M_J
\end{align*}
\item consider polynomial models with multiple predictors $\{X_1, X_2\}$ and cross terms:
\begin{align*}
y = & \beta_0 + \beta_1 x_1 + \ldots + \beta_M x^M_1\\
&+ \beta_{1 + M} x_2 + \ldots + \beta_{2M} x^M_2 \\
&+ \beta_{1 + 2M} (x_1x_2) + \ldots + \beta_{3M} (x_1x_2)^M
\end{align*}
\end{enumerate}
In each case, we consider each term $x^m_j$ and each cross term $x_1x_2$ an unique predictor and apply linear regression.
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bibliography}

\begin{enumerate}
 \fontsize{8}{10}\selectfont
\item Bolelli, L., Ertekin, S., and Giles, C. L. \emph{Topic and trend detection in text collections using latent dirichlet allocation}. In European Conference on Information Retrieval (2009), Springer, pp. 776-780.
\item Chen, W., Wang, Y., and Yang, S. \emph{Efficient influence maximization in social networks. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (2009)}, ACM, pp. 199-208.
\item Chong, W., Blei, D., and Li, F.-F. \emph{Simultaneous image classification and annotation. In Computer Vision and Pattern Recognition}, 2009. CVPR 2009. IEEE Conference on (2009), IEEE, pp. 1903-1910.
\item Du, L., Ren, L., Carin, L., and Dunson, D. B. \emph{A bayesian model for simultaneous image clustering, annotation and object segmentation}. In Advances in neural information processing systems (2009), pp. 486-494.
\item Elango, P. K., and Jayaraman, K. \emph{Clustering images using the latent dirichlet allocation model.}
\item Feng, Y., and Lapata, M. \emph{Topic models for image annotation and text illustration}. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (2010), Association for Computational Linguistics, pp. 831-839.
\item Hannah, L. A., and Wallach, H. M. \emph{Summarizing topics: From word lists to phrases.}
\item Lu, R., and Yang, Q. \emph{Trend analysis of news topics on twitter}. International Journal of Machine Learning and Computing 2, 3 (2012), 327.
\end{enumerate}

\end{frame}

\end{document}
